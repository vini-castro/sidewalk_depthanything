{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vini-castro/sidewalk_depthanything/blob/main/sidealkcolab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9gMt1KvREuv"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import pkg_resources\n",
        "\n",
        "def get_numpy_version():\n",
        "    try:\n",
        "        return pkg_resources.get_distribution(\"numpy\").version\n",
        "    except pkg_resources.DistributionNotFound:\n",
        "        return None\n",
        "\n",
        "current_version = get_numpy_version()\n",
        "print(current_version)\n",
        "\n",
        "target_version = \"1.23.1\"\n",
        "\n",
        "if current_version != target_version:\n",
        "    print(f\"Numpy versão {current_version} detectada. Atualizando para {target_version}...\")\n",
        "\n",
        "    !pip uninstall -y numpy\n",
        "    !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1\n",
        "\n",
        "    print(\"Atualização concluída. Reinicie o ambiente\")\n",
        "else:\n",
        "    print(f\"Numpy já está na versão {target_version}. Nenhuma ação necessária.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUmK767kWQlq"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!rm -rf OneFormer/\n",
        "!git clone https://github.com/SHI-Labs/OneFormer-Colab.git\n",
        "! mv OneFormer-Colab OneFormer\n",
        "%cd /content/OneFormer/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGz79OJwNDxq"
      },
      "outputs": [],
      "source": [
        "import sys, os, distutils.core\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "!git clone 'https://github.com/vini-castro/sidewalk_depthanything' -q\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])} --quiet\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzVUqlOwHOBa"
      },
      "outputs": [],
      "source": [
        "!pip3 install -U opencv-python --quiet\n",
        "!pip3 install natten==0.14.6  --quiet\n",
        "\n",
        "!pip3 install -r requirements.txt --quiet\n",
        "!pip3 install ipython-autotime --quiet\n",
        "!pip3 install imutils --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw41v_NH8vM9"
      },
      "outputs": [],
      "source": [
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "setup_logger(name=\"oneformer\")\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import math\n",
        "from google.colab.patches import cv2_imshow\n",
        "import imutils\n",
        "\n",
        "# Import detectron2 utilities\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.projects.deeplab import add_deeplab_config\n",
        "from detectron2.data import MetadataCatalog\n",
        "from demo.defaults import DefaultPredictor\n",
        "from demo.visualizer import Visualizer, ColorMode\n",
        "\n",
        "\n",
        "# import OneFormer Project\n",
        "from oneformer import (\n",
        "    add_oneformer_config,\n",
        "    add_common_config,\n",
        "    add_swin_config,\n",
        "    add_dinat_config,\n",
        "    add_convnext_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLH-GzQCxw53"
      },
      "outputs": [],
      "source": [
        "cpu_device = torch.device(\"cpu\")\n",
        "SWIN_CFG_DICT = {\"cityscapes\": \"configs/cityscapes/oneformer_swin_large_IN21k_384_bs16_90k.yaml\",\n",
        "            \"ade20k\": \"configs/ade20k/oneformer_swin_large_IN21k_384_bs16_160k.yaml\",}\n",
        "\n",
        "DINAT_CFG_DICT = {\"cityscapes\": \"configs/cityscapes/oneformer_dinat_large_bs16_90k.yaml\",\n",
        "            \"ade20k\": \"configs/ade20k/oneformer_dinat_large_IN21k_384_bs16_160k.yaml\",}\n",
        "\n",
        "def setup_cfg(dataset, model_path, use_swin):\n",
        "    cfg = get_cfg()\n",
        "    add_deeplab_config(cfg)\n",
        "    add_common_config(cfg)\n",
        "    add_swin_config(cfg)\n",
        "    add_dinat_config(cfg)\n",
        "    add_convnext_config(cfg)\n",
        "    add_oneformer_config(cfg)\n",
        "    if use_swin:\n",
        "      cfg_path = SWIN_CFG_DICT[dataset]\n",
        "    else:\n",
        "      cfg_path = DINAT_CFG_DICT[dataset]\n",
        "    cfg.merge_from_file(cfg_path)\n",
        "    cfg.MODEL.DEVICE = 'cpu'\n",
        "    cfg.MODEL.WEIGHTS = model_path\n",
        "    cfg.freeze()\n",
        "    return cfg\n",
        "\n",
        "def setup_modules(dataset, model_path, use_swin):\n",
        "    cfg = setup_cfg(dataset, model_path, use_swin)\n",
        "    predictor = DefaultPredictor(cfg)\n",
        "    metadata = MetadataCatalog.get(\n",
        "        cfg.DATASETS.TEST_PANOPTIC[0] if len(cfg.DATASETS.TEST_PANOPTIC) else \"__unused\"\n",
        "    )\n",
        "    if 'cityscapes_fine_sem_seg_val' in cfg.DATASETS.TEST_PANOPTIC[0]:\n",
        "        from cityscapesscripts.helpers.labels import labels\n",
        "        stuff_colors = [k.color for k in labels if k.trainId != 255]\n",
        "        metadata = metadata.set(stuff_colors=stuff_colors)\n",
        "\n",
        "    return predictor, metadata\n",
        "\n",
        "def panoptic_run(img, predictor, metadata):\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, instance_mode=ColorMode.IMAGE)\n",
        "    predictions = predictor(img, \"panoptic\")\n",
        "    panoptic_seg, segments_info = predictions[\"panoptic_seg\"]\n",
        "    out = visualizer.draw_panoptic_seg_predictions(\n",
        "    panoptic_seg.to(cpu_device), segments_info, alpha=0.5\n",
        ")\n",
        "    return out\n",
        "\n",
        "def instance_run(img, predictor, metadata):\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, instance_mode=ColorMode.IMAGE)\n",
        "    predictions = predictor(img, \"instance\")\n",
        "    instances = predictions[\"instances\"].to(cpu_device)\n",
        "    out = visualizer.draw_instance_predictions(predictions=instances, alpha=0.5)\n",
        "    return out\n",
        "\n",
        "def semantic_run(img, predictor, metadata):\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=metadata, instance_mode=ColorMode.IMAGE)\n",
        "    predictions = predictor(img, \"semantic\")\n",
        "    out = visualizer.draw_sem_seg(\n",
        "        predictions[\"sem_seg\"].argmax(dim=0).to(cpu_device), alpha=0.5\n",
        "    )\n",
        "    return out\n",
        "\n",
        "def calcula_coeficiente(lat_orig, long_orig, lat_dest, long_dest):\n",
        "    lat1 = math.radians(lat_orig)\n",
        "    lon1 = math.radians(long_orig)\n",
        "    lat2 = math.radians(lat_dest)\n",
        "    lon2 = math.radians(long_dest)\n",
        "\n",
        "    delta_lon = lon2 - lon1\n",
        "\n",
        "    x = math.cos(lat2) * math.sin(delta_lon)\n",
        "    y = math.cos(lat1) * math.sin(lat2) - math.sin(lat1) * math.cos(lat2) * math.cos(delta_lon)\n",
        "\n",
        "    initial_bearing = math.atan2(x, y)\n",
        "    initial_bearing = math.degrees(initial_bearing)\n",
        "\n",
        "    #90 graus para olhar a calçada de frente\n",
        "    bearing = (initial_bearing + 360 - 90 ) % 360\n",
        "\n",
        "    return bearing\n",
        "\n",
        "TASK_INFER = {\"panoptic\": panoptic_run,\n",
        "              \"instance\": instance_run,\n",
        "              \"semantic\": semantic_run}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX8TLytOVal8"
      },
      "outputs": [],
      "source": [
        "######\n",
        "#@markdown We use `DiNAT-L` as the default backbone. To use Swin-L as backbone, select the checkbox below.\n",
        "use_swin = True #@param {type: 'boolean'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ldq3OJUzVl2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "if not use_swin:\n",
        "  if not os.path.exists(\"250_16_dinat_l_oneformer_ade20k_160k.pth\"):\n",
        "    subprocess.run('wget https://shi-labs.com/projects/oneformer/ade20k/250_16_dinat_l_oneformer_ade20k_160k.pth', shell=True)\n",
        "  predictor, metadata = setup_modules(\"ade20k\", \"250_16_dinat_l_oneformer_ade20k_160k.pth\", use_swin)\n",
        "else:\n",
        "  if not os.path.exists(\"250_16_swin_l_oneformer_ade20k_160k.pth\"):\n",
        "    subprocess.run('wget https://shi-labs.com/projects/oneformer/ade20k/250_16_swin_l_oneformer_ade20k_160k.pth', shell=True)\n",
        "  predictor, metadata = setup_modules(\"ade20k\", \"250_16_swin_l_oneformer_ade20k_160k.pth\", use_swin)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/OneFormer/sidewalk_depthanything/torchhub /content/OneFormer/"
      ],
      "metadata": {
        "id": "5W3x5KLyTskC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "QyBdCW7VUE3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Caso não queira usar a API do Google Maps, insira as imagens desejadas, ou use as de exemplos em OneFormer/sideawlk_depthanything/assets/examples\n",
        "#Caso queira usar a API do GoogleMaps, é necessário uma API_KEY valida\n",
        "\n",
        "API_KEY = ''\n",
        "img_path_list = ['./sidewalk_depthanything/assets/examples/passo1.png', './sidewalk_depthanything/assets/examples/passo2.png', './sidewalk_depthanything/assets/examples/passo3.png']"
      ],
      "metadata": {
        "id": "4dzEc--ZHgqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ay7FxQcf_0r"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# from PIL import Image\n",
        "# from io import BytesIO\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# origem = 'R. Antônio de Macedo Soares, 1399'\n",
        "# destino = 'R. Vieira de Morais, 754'\n",
        "\n",
        "# geocode_url_origem = f'https://maps.googleapis.com/maps/api/geocode/json?address={origem}&key={API_KEY}'\n",
        "# geocode_url_destino = f'https://maps.googleapis.com/maps/api/geocode/json?address={destino}&key={API_KEY}'\n",
        "# geocode_response_origem = requests.get(geocode_url_origem)\n",
        "# geocode_response_destino = requests.get(geocode_url_destino)\n",
        "\n",
        "# if geocode_response_origem.status_code == 200 and geocode_response_destino.status_code == 200:\n",
        "#     geocode_data_origem = geocode_response_origem.json()\n",
        "#     geocode_data_destino = geocode_response_destino.json()\n",
        "\n",
        "#     if geocode_data_origem['status'] == 'OK' and geocode_data_destino['status'] == 'OK':\n",
        "\n",
        "#         latitude_origem = geocode_data_origem['results'][0]['geometry']['location']['lat']\n",
        "#         longitude_origem = geocode_data_origem['results'][0]['geometry']['location']['lng']\n",
        "\n",
        "#         latitude_destino = geocode_data_destino['results'][0]['geometry']['location']['lat']\n",
        "#         longitude_destino = geocode_data_destino['results'][0]['geometry']['location']['lng']\n",
        "\n",
        "#         print(f\"Origem (Coordenadas): {latitude_origem}, {longitude_origem}\")\n",
        "#         print(f\"Destino (Coordenadas): {latitude_destino}, {longitude_destino}\")\n",
        "\n",
        "#         directions_url = f'https://maps.googleapis.com/maps/api/directions/json?origin={latitude_origem},{longitude_origem}&destination={latitude_destino},{longitude_destino}&key={API_KEY}'\n",
        "#         directions_response = requests.get(directions_url)\n",
        "\n",
        "#         if directions_response.status_code == 200:\n",
        "#             directions_data = directions_response.json()\n",
        "\n",
        "#             if directions_data['status'] == 'OK':\n",
        "\n",
        "#                 polyline = directions_data['routes'][0]['overview_polyline']['points']\n",
        "\n",
        "#                 size = '600x600'\n",
        "#                 map_type = 'roadmap'\n",
        "#                 color = '0x0000FF'\n",
        "#                 weight = '5'\n",
        "\n",
        "#                 static_map_url = f'https://maps.googleapis.com/maps/api/staticmap?size={size}&maptype={map_type}&path=color:{color}|weight:{weight}|enc:{polyline}&key={API_KEY}'\n",
        "\n",
        "#                 response = requests.get(static_map_url)\n",
        "\n",
        "#                 if response.status_code == 200:\n",
        "#                     image = Image.open(BytesIO(response.content))\n",
        "#                     plt.imshow(image)\n",
        "#                     plt.axis('off')\n",
        "#                     plt.show()\n",
        "#                 else:\n",
        "#                     print('Erro ao obter a imagem do mapa:', response.status_code)\n",
        "#             else:\n",
        "#                 print('Erro ao obter as direções:', directions_data['status'])\n",
        "#         else:\n",
        "#             print('Erro ao obter a rota:', directions_response.status_code)\n",
        "#     else:\n",
        "#         print('Erro na geocodificação:', geocode_data_origem['status'], geocode_data_destino['status'])\n",
        "# else:\n",
        "#     print('Erro ao obter a geocodificação:', geocode_response_origem.status_code, geocode_response_destino.status_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from PIL import Image\n",
        "# from io import BytesIO\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# origem = 'R. Antônio de Macedo Soares, 1399'\n",
        "# destino = 'R. Vieira de Morais, 754'\n",
        "\n",
        "# geocode_url_origem = f'https://maps.googleapis.com/maps/api/geocode/json?address={origem}&key={API_KEY}'\n",
        "# geocode_url_destino = f'https://maps.googleapis.com/maps/api/geocode/json?address={destino}&key={API_KEY}'\n",
        "\n",
        "# geocode_response_origem = requests.get(geocode_url_origem)\n",
        "# geocode_response_destino = requests.get(geocode_url_destino)\n",
        "\n",
        "# img_path_list = []\n",
        "# if geocode_response_origem.status_code == 200 and geocode_response_destino.status_code == 200:\n",
        "#     geocode_data_origem = geocode_response_origem.json()\n",
        "#     geocode_data_destino = geocode_response_destino.json()\n",
        "\n",
        "#     if geocode_data_origem['status'] == 'OK' and geocode_data_destino['status'] == 'OK':\n",
        "#         latitude_origem = geocode_data_origem['results'][0]['geometry']['location']['lat']\n",
        "#         longitude_origem = geocode_data_origem['results'][0]['geometry']['location']['lng']\n",
        "\n",
        "#         latitude_destino = geocode_data_destino['results'][0]['geometry']['location']['lat']\n",
        "#         longitude_destino = geocode_data_destino['results'][0]['geometry']['location']['lng']\n",
        "\n",
        "#         directions_url = f'https://maps.googleapis.com/maps/api/directions/json?origin={latitude_origem},{longitude_origem}&destination={latitude_destino},{longitude_destino}&key={API_KEY}'\n",
        "#         directions_response = requests.get(directions_url)\n",
        "\n",
        "#         if directions_response.status_code == 200:\n",
        "#             directions_data = directions_response.json()\n",
        "\n",
        "#             if directions_data['status'] == 'OK':\n",
        "#                 steps = directions_data['routes'][0]['legs'][0]['steps']\n",
        "\n",
        "#                 for i in range(len(steps)-1):\n",
        "#                     start_lat = steps[i]['start_location']['lat']\n",
        "#                     start_lng = steps[i]['start_location']['lng']\n",
        "#                     end_lat = steps[i+1]['start_location']['lat']\n",
        "#                     end_lng = steps[i+1]['start_location']['lng']\n",
        "\n",
        "#                     middle_lat = (start_lat + end_lat) / 2\n",
        "#                     middle_lng = (start_lng + end_lng) / 2\n",
        "\n",
        "#                     heading = calcula_coeficiente(start_lat, start_lng, end_lat, end_lng)\n",
        "#                     pitch = -10\n",
        "\n",
        "#                     street_view_url = f'https://maps.googleapis.com/maps/api/streetview?size=600x600&location={middle_lat},{middle_lng}&heading={heading}&pitch={pitch}&key={API_KEY}'\n",
        "\n",
        "#                     response = requests.get(street_view_url)\n",
        "\n",
        "#                     if response.status_code == 200:\n",
        "#                         print(f\"Mostrando imagem para o passo {i+1}\")\n",
        "#                         img_step = Image.open(BytesIO(response.content))\n",
        "#                         img_step.save(f'passo{i+1}.png')\n",
        "#                         img_path_list.append(f'passo{i+1}.png')\n",
        "\n",
        "#                         plt.imshow(img_step)\n",
        "#                         plt.axis('off')\n",
        "#                         plt.show()\n",
        "#                     else:\n",
        "#                         print(f'Erro ao obter a imagem do passo {i+1}:', response.status_code)\n",
        "#             else:\n",
        "#                 print('Erro ao obter as direções:', directions_data['status'])\n",
        "#         else:\n",
        "#             print('Erro ao obter a rota:', directions_response.status_code)\n",
        "#     else:\n",
        "#         print('Erro na geocodificação:', geocode_data_origem['status'], geocode_data_destino['status'])\n",
        "# else:\n",
        "#     print('Erro ao obter a geocodificação:', geocode_response_origem.status_code, geocode_response_destino.status_code)\n"
      ],
      "metadata": {
        "id": "9pW6R8Z26TWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUjkwRsOn1O0"
      },
      "outputs": [],
      "source": [
        "task = \"panoptic\"\n",
        "def execute_prediction(img_path, show_img):\n",
        "  img = cv2.resize(cv2.imread(img_path), (600, 600))\n",
        "  predictions = predictor(img, \"panoptic\")\n",
        "\n",
        "  id_calcada = -1\n",
        "  for item in predictions[\"panoptic_seg\"][1]:\n",
        "      if item['category_id'] == 11:\n",
        "          id_calcada = item['id']\n",
        "          break\n",
        "  if(show_img):\n",
        "    img = np.clip(img, 1, 254)\n",
        "    out = TASK_INFER[task](img, predictor, metadata).get_image()\n",
        "    cv2_imshow(out[:, :, ::-1])\n",
        "  return predictions, id_calcada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMwt8QYbTram"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sidewalk_depthanything.pixel_size import pixel_sum\n",
        "from torchvision.transforms import Compose\n",
        "from tqdm import tqdm\n",
        "from sidewalk_depthanything.depth_anything.dpt import DepthAnything\n",
        "from sidewalk_depthanything.depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n",
        "\n",
        "import sys\n",
        "for idx, arg in enumerate(sys.argv):\n",
        "    if arg == '-f':\n",
        "        sys.argv.pop(idx)\n",
        "        sys.argv.pop(idx)\n",
        "\n",
        "parser = argparse.ArgumentParser(allow_abbrev=False)\n",
        "parser.add_argument('--outdir', type=str, default='depth_vis')\n",
        "parser.add_argument('--encoder', type=str, default='vitl', choices=['vits', 'vitb', 'vitl'])\n",
        "parser.add_argument('--pred-only', dest='pred_only', action='store_true', help='only display the prediction')\n",
        "parser.add_argument('--grayscale', dest='grayscale', action='store_true', help='do not apply colorful palette')\n",
        "args = parser.parse_args()\n",
        "\n",
        "margin_width = 50\n",
        "caption_height = 60\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "font_scale = 1\n",
        "font_thickness = 2\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(args.encoder)\n",
        "depth_anything = DepthAnything.from_pretrained('LiheYoung/depth_anything_{}14'.format(args.encoder)).to(DEVICE).eval()\n",
        "\n",
        "total_params = sum(param.numel() for param in depth_anything.parameters())\n",
        "print('Total parameters: {:.2f}M'.format(total_params / 1e6))\n",
        "\n",
        "transform = Compose([\n",
        "  Resize(\n",
        "      width=518,\n",
        "      height=518,\n",
        "      resize_target=False,\n",
        "      keep_aspect_ratio=True,\n",
        "      ensure_multiple_of=14,\n",
        "      resize_method='lower_bound',\n",
        "      image_interpolation_method=cv2.INTER_CUBIC,\n",
        "  ),\n",
        "  NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "  PrepareForNet(),\n",
        "])\n",
        "\n",
        "def process_image(img_path):\n",
        "  if os.path.isfile(img_path):\n",
        "    if img_path.endswith('txt'):\n",
        "        with open(img_path, 'r') as f:\n",
        "            filenames = f.read().splitlines()\n",
        "    else:\n",
        "        filenames = [img_path]\n",
        "  else:\n",
        "    filenames = os.listdir(img_path)\n",
        "    filenames = [os.path.join(img_path, filename) for filename in filenames if not filename.startswith('.')]\n",
        "    filenames.sort()\n",
        "\n",
        "  os.makedirs(args.outdir, exist_ok=True)\n",
        "\n",
        "  for filename in tqdm(filenames):\n",
        "    raw_image = cv2.resize(cv2.imread(filename), (600, 600))\n",
        "    image = cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB) / 255.0\n",
        "\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    image = transform({'image': image})['image']\n",
        "    image = torch.from_numpy(image).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        depth = depth_anything(image)\n",
        "\n",
        "    depth = F.interpolate(depth[None], (h, w), mode='bilinear', align_corners=False)[0, 0]\n",
        "    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
        "    depth_norm = (depth - 1) / 254\n",
        "    depth = depth.cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    value_bottom = depth_norm[h-4, w//2]\n",
        "    value_above = depth_norm[h-5, w//2]\n",
        "    pixel_size = 175/(value_above**2) - 175/(value_bottom**2)\n",
        "    print(f'pixel_size = {pixel_size}')\n",
        "\n",
        "    object_width_cm = pixel_size / depth_norm\n",
        "\n",
        "    np.set_printoptions(threshold=np.inf, linewidth=np.inf)\n",
        "\n",
        "    if args.grayscale:\n",
        "        depth = np.repeat(depth[..., np.newaxis], 3, axis=-1)\n",
        "    else:\n",
        "        depth = cv2.applyColorMap(depth, cv2.COLORMAP_INFERNO)\n",
        "\n",
        "    filename = os.path.basename(filename)\n",
        "\n",
        "    if args.pred_only:\n",
        "        cv2.imwrite(os.path.join(args.outdir, filename[:filename.rfind('.')] + '_depth.png'), depth)\n",
        "    else:\n",
        "        split_region = np.ones((raw_image.shape[0], margin_width, 3), dtype=np.uint8) * 255\n",
        "        combined_results = cv2.hconcat([raw_image, split_region, depth])\n",
        "\n",
        "        caption_space = np.ones((caption_height, combined_results.shape[1], 3), dtype=np.uint8) * 255\n",
        "        captions = ['Raw image', 'Depth Anything']\n",
        "        segment_width = w + margin_width\n",
        "\n",
        "        for i, caption in enumerate(captions):\n",
        "            text_size = cv2.getTextSize(caption, font, font_scale, font_thickness)[0]\n",
        "\n",
        "            text_x = int((segment_width * i) + (w - text_size[0]) / 2)\n",
        "\n",
        "            cv2.putText(caption_space, caption, (text_x, 40), font, font_scale, (0, 0, 0), font_thickness)\n",
        "\n",
        "        final_result = cv2.vconcat([caption_space, combined_results])\n",
        "\n",
        "        cv2.imwrite(os.path.join(args.outdir, filename[:filename.rfind('.')] + '_img_depth.png'), final_result)\n",
        "    return object_width_cm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use esse metodo quando a calçada estiver na Verical\n",
        "import numpy as np\n",
        "\n",
        "count = 0\n",
        "for img in img_path_list:\n",
        "  object_width_cm = process_image(img)\n",
        "  panoptic_seg, id_calcada = execute_prediction(img, True) #Utilize False para não mostrar a imagem e reduzir a chacne de erros\n",
        "  panoptic_seg = panoptic_seg[\"panoptic_seg\"][0]\n",
        "\n",
        "  count += 1\n",
        "  distancias_verticais = []\n",
        "\n",
        "  for j in range(0, panoptic_seg.shape[1]):\n",
        "      linha = panoptic_seg[:, j]\n",
        "\n",
        "      soma = 0\n",
        "      for i in range(int(panoptic_seg.shape[0]/3), panoptic_seg.shape[0]):\n",
        "          if linha[i] == id_calcada:\n",
        "              soma += object_width_cm[i][j]\n",
        "      if soma != 0:\n",
        "          distancias_verticais.append(soma)\n",
        "  print(f'Media da calçada da img {count}: {np.mean(distancias_verticais)}')"
      ],
      "metadata": {
        "id": "q_8tcfoSsiw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0rxep0UU0Ei"
      },
      "outputs": [],
      "source": [
        "#Use esse método quando a calçada estiver Paralela\n",
        "count = 0\n",
        "for img in img_path_list:\n",
        "    count += 1\n",
        "    object_width_cm = process_image(img)\n",
        "    distancias_direita = []\n",
        "    distancias_esquerda = []\n",
        "    panoptic_seg, id_calcada = execute_prediction(img, True)\n",
        "    panoptic_seg = panoptic_seg[\"panoptic_seg\"][0]\n",
        "    meio = panoptic_seg.shape[1] // 2\n",
        "\n",
        "    for j in range(int(panoptic_seg.shape[0]/3), panoptic_seg.shape[0]):\n",
        "        linha = panoptic_seg[j, :]\n",
        "\n",
        "        soma = 0\n",
        "        for i in range(meio, panoptic_seg.shape[1]):\n",
        "            if linha[i] == id_calcada:\n",
        "                soma += object_width_cm[j][i]\n",
        "\n",
        "        if soma != 0:\n",
        "            distancias_direita.append(soma)\n",
        "    print(f'Media da direita da imagem {count}: {np.mean(distancias_direita)}')\n",
        "\n",
        "    for j in range(int(panoptic_seg.shape[:][0]/3), panoptic_seg.shape[:][0]):\n",
        "        linha = panoptic_seg[j, :]\n",
        "\n",
        "        soma = 0\n",
        "        for i in range(0, meio):\n",
        "            if linha[i] == id_calcada:\n",
        "                soma += object_width_cm[j][i]\n",
        "        if soma != 0:\n",
        "            distancias_esquerda.append(soma)\n",
        "\n",
        "    print(f'Media da esquerda da imagem {count}: {np.mean(distancias_esquerda)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exemplo de profundidade\n",
        "img_depth = cv2.imread('./depth_vis/passo1_img_depth.png')\n",
        "cv2_imshow(img_depth)"
      ],
      "metadata": {
        "id": "GWpIdVbzaSwd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}